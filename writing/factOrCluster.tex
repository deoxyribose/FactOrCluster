\documentclass{article}
\input{preambles/basicmath}
\input{preambles/extrasymbols}

\newcommand{\mix}{\bm{A}}
\newcommand{\mixvec}{\bm{a}}
\newcommand{\source}{s}
\newcommand{\sourcevec}{\bm{\source}}
\newcommand{\obs}{y}
\newcommand{\obsvec}{\bm{\obs}}

\newcommand{\noisecov}{\bm{\Lambda}}
\newcommand{\noisescalesq}{\lambda^2}




\newcommand{\obsmean}{\bar{\mu}}
\newcommand{\obsmeanvec}{\bm{\obsmean}}

\newcommand{\obscov}{\bm{\bar{\Sigma}}}

\newcommand{\mean}{\mu}
\newcommand{\meanvec}{\bm{\mean}}
\newcommand{\scale}{\sigma}
\newcommand{\scalesq}{\scale^2}
\newcommand{\weight}{w}
\newcommand{\weightvec}{\bm{\weight}}

\newcommand{\subweight}{\hat{w}}
\newcommand{\submean}{\hat{\mu}}
\newcommand{\subscale}{\hat{\sigma}}
\newcommand{\subweightvec}{\bm{\subweight}}
\newcommand{\submeanvec}{\bm{\submean}}
\newcommand{\subscalesq}{\subscale^2}

\newcommand{\compweight}{{\weight}}
\newcommand{\compweightvec}{{\bm{\weight}}}
\newcommand{\compmean}{{\mean}}
\newcommand{\compmeanvec}{{\meanvec}}
\newcommand{\compcov}{{\bm{\Sigma}}}
\newcommand{\compscalesq}{{{\scale}}^2}


\newcommand{\assign}{z}
\newcommand{\assignvec}{\bm{\assign}}


\newcommand{\numobs}{N}
\newcommand{\numfactor}{S}
\newcommand{\numcluster}{C}
\newcommand{\numifa}{Q}


\newcommand{\icluster}{c}
\newcommand{\iifa}{q}
\newcommand{\ifactor}{s}

\newcommand{\obsdim}{D}

\title{Distinguishing independent factors from clusters}

\begin{document}
\maketitle

\begin{abstract}
It can be difficult to discern whether variation in a population is due to independent interacting factors, or because there are multiple subtypes within the population with their own patterns of variation. 
We propose using probabilistic generative models to evaluate whether it is more likely that any given dataset is generated by independent factors, as modeled by the Independent Factor Analysis model, or a cluster-based model, in the form of a Gaussian mixture model.  
\end{abstract}

\section{Factor and Mixture Models}
Factor models are a very common choice for modeling structured variation, and in particular for identifying orthogonal or independent factors describing different modes of variation. In this section we will use the probabilistic interpretation of this class of models.

In the most general setting, we assume $\numobs$ observations $\obsvec_n\in \reals^\obsdim$ are generated by the projection of a latent source vector $\sourcevec_n\in \reals^\numfactor$ via a mixing matrix $\mix \in \reals^{\obsdim\times\numfactor}$ and the addition of Gaussian noise with covariance matrix $\noisecov\in\mathcal{S}_+^{\numfactor\times\numfactor}$ as,
\begin{equation}
\label{eq:likelihood}
\obsvec_n\cond\sourcevec_n,\mix,\noisecov\sim  \gaussrnd(\mix\sourcevec_n,\noisecov).
\end{equation}
By imposing different constraints and priors on $\sourcevec_n$, $\mix$ and $\noisecov$, we sweep out a number of well-known models. If $\sourcevec_n\sim\gaussrnd(\bm{0},\bm{I})$, then we can marginalize out the source analytically, resulting in a Gaussian marginal density for $\obsvec_n$ of form,
\begin{equation}
\label{eq:factormodel}
\obsvec_n\cond \mix, \noisecov\sim \gaussrnd(\bm{0},\mix\mix\T+\noisecov).
\end{equation}
If we assume that $\noisecov$ is diagonal, we get what is known as the probabilistic factor analysis model (PFA) in the literature. If we assume that it is not just diagonal, but a scaled identity like $\noisecov=\noisescalesq\bm{I}$, we retrieve the probabilistic principal component analysis model (PPCA).

Models with this type of covariance structure describe densities with ellipsoidal contours, with the factors $U$ of their eigen-decomposition $UVU\T=\mix\mix\T+\noisecov$ describing the axes of the ellipsoid, and the eigenvalues $V$ determining their respective lengths. If $\noisecov$ goes to $0$, the density concentrates in the low-dimensional hyperplane spanned by the column vectors of $\mix$.

Since these densities remain ellipsoidal, it might be necessary to assume a more complicated density than a standard normal on the source vector to adequately model the data. Since we can marginalize over Gaussians, a convenient choice is a mixture of Gaussians defined as,
\begin{equation}
\label{eq:sourcemixture}
\sourcevec_n\cond \lbrace \compweight_\icluster,\compmeanvec_\icluster,\compcov_\icluster\rbrace_{\icluster=1}^{\numcluster}\sim \sum_{\icluster=1}^{\numcluster}\compweight_\icluster\gaussrnd(\compmeanvec_\icluster,\compcov_\icluster)
\end{equation}
where in a slight abuse of notation, we have used the sum over distributions to describe a distribution with the mixture density,
\begin{equation}
p(\sourcevec_n\cond \lbrace \compweight_\icluster,\compmeanvec_\icluster,\compcov_\icluster\rbrace_{\icluster=1}^{\numcluster})=\sum_{\icluster=1}^{\numcluster}\compweight_\icluster\frac{|\compcov_\icluster|^{-1/2}}{(2\pi)^{\numfactor/2}}e^{-\frac{1}{2}(\sourcevec_n-\compmeanvec_\icluster)\T\compcov_\icluster^{-1}(\sourcevec_n-\compmeanvec_\icluster)}. 
\end{equation}
Using this density on the source space leads to a model we will call the projected mixture of Gaussians (projMoG), which is related to the so-called mixture of factor analyzers (MoFA), but is comparatively weaker as it does not allow the mixing matrix $\mix$ to differ between the clusters. Instead, it describes a standard Gaussian mixture that is restricted to the hyperspace spanned by $\mix$,
\begin{equation}
\label{eq:obsmixture}
\obsvec_n\cond \mix,\noisecov,\lbrace \compweight_\icluster,\compmeanvec_\icluster,\compcov_\icluster\rbrace_{\icluster=1}^{\numcluster}\sim \sum_{\icluster=1}^{\numcluster}\compweight_\icluster\gaussrnd(\mix\compmeanvec_\icluster,\mix\compcov_\icluster\mix\T+\noisecov).
\end{equation}

The projected mixture is a good fit for modeling cluster-based variation, but does not model independent variation along the factors specified by $\mix$ since its Gaussian components span over all the dimensions of the subspace. To constrain the model further, we can assume that each element of the source vector is drawn from its own univariate mixture,
\begin{equation}
\label{eq:submixture}
\source_{n\ifactor}\cond \lbrace \subweight_{\ifactor\iifa},\submean_{\ifactor\iifa},\subscalesq_{\ifactor\iifa}\rbrace_{\iifa=1}^{\numifa} \sim\sum_{\iifa=1}^{\numifa}\subweight_{\ifactor\iifa}\gaussrnd(\submean_{\ifactor\iifa},\subscalesq_{\ifactor\iifa}).
\end{equation}
By multiplying together these univariate densities, we can find the corresponding multivariate mixture of a form like in equation \eqref{eq:sourcemixture} but with parameters,
\begin{equation}
\compweight_{\icluster}=\prod_{\ifactor=1}^{\numfactor}\prod_{\iifa=1}^{\numifa}\subweight_{\ifactor\iifa}^{\xi_{\icluster}^{\ifactor\iifa}},\quad \compmean_{\icluster\ifactor}=\sum_{\iifa=1}^{\numifa}\xi_{\icluster}^{\ifactor\iifa}\submean_{\ifactor\iifa},\quad \scalesq_{\icluster\ifactor}=\sum_{\iifa=1}^{\numifa}\xi_{\icluster}^{\ifactor\iifa}\subscalesq_{\ifactor\iifa}. 
\end{equation}
Here, the clusters indexed by $\icluster$ are associated with the 
$\numcluster=\numifa^{\numfactor}$ combinatorial combinations of the components in the sub-mixtures, with $\xi_{\icluster}^{\ifactor\iifa}$ equal to $1$ if the $\iifa$'th component of the mixture governing factor $\ifactor$ is involved in cluster $\icluster$, or $0$ otherwise. 

This model is known as independent factor analysis (IFA) and we further define the centered IFA (cIFA) as being the variant where $\mean_{\ifactor\iifa}=0$ for all sub-components. These models are weaker than the projected Gaussian mixture as they are limited to a factorial mixture on the sources. On the other hand, they are closely related to models like the (probabilistic) independent component analysis model (ICA) which nominally employs independent non-Gaussian source distributions, such as the Laplace distribution. Many such non-Gaussian densities can be modeled or approximated by a univariate mixture of Gaussians like in equation \eqref{eq:submixture}, making IFA a tractable alternative to such models (although the $\numcluster=\numifa^{\numfactor}$ scaling means that factors and sub-components have to be limited in number). While Gaussian source distributions induce a Gaussian density in the observation space, ICA is appropriate for modeling data where only a few modes of variation are expressed for any one observation, such as data that concentrates along multiple one-dimensional subspaces in a star-like configuration.

\appendix
\section{Appendix}

\subsection{Calibration} 
Let use first define the means and covariances of each induced cluster component on the observation space following the mixture in equation \eqref{eq:obsmixture},
\begin{equation}
\obsmeanvec_\icluster=\mix \compmeanvec_\icluster,\quad \obscov_{\icluster}=\mix\compcov_\icluster\mix\T+\noisecov
\end{equation}


We can calculate the first two moments of all the models we have discussed so far, which can be described by the general form of equation \eqref{eq:obsmixture},
\begin{align*}
\E{\obsvec_n\cond \mix,\noisecov,\lbrace \compweight_\icluster,\compmeanvec_\icluster,\compcov_\icluster\rbrace_{\icluster=1}^{\numcluster}}&=\sum_{\icluster=1}^{\numcluster}\compweight_{\icluster}\obsmeanvec_{\icluster}\\
\E{\obsvec_n\obsvec_n\T\cond \mix,\noisecov,\lbrace \compweight_\icluster,\compmeanvec_\icluster,\compcov_\icluster\rbrace_{\icluster=1}^{\numcluster}}&=\sum_{\icluster=1}^{\numcluster}\compweight_\icluster\left(\obscov_\icluster + \obsmeanvec_{\icluster}\obsmeanvec_{\icluster}\T\right)
\end{align*}

Using the tower property, we can then get the first and second moment of the marginal distribution over $\obsvec$ by taking the expectation with respect to the remaining parameters, which are all independent.
\begin{align}
\E{\obsvec_n}&=\sum_{\icluster=1}^{\numcluster}\E{\compweight_\icluster}\E{\mix}\E{\compmeanvec_\icluster}\\
\E{\obsvec_n\obsvec_n\T}&=\sum_{\icluster=1}^{\numcluster}\E{\compweight_\icluster}\left(\E{\mix(\E{\compcov_\icluster}+\E{\compmeanvec_\icluster\compmeanvec_\icluster\T})\mix\T}+\E{\noisecov}\right)
\end{align}
These expectations reduce to means and second moments of atomic random variables, except for the expectation involving the two mixing matrices $\mix$ which is confounded by the interspersed factor.

We simplify the above expression considerably by first assuming that $\E{\compmeanvec_\icluster}=0$, which is reasonable for centered data and sets $\E{\obsvec_n}=\bm{0}$, which in turn identifies the covariance of $\obsvec_n$ with its second moment. We then set the covariances to be scaled identities, so $\E{\compcov_{\icluster}}=m_{\scalesq}\bm{I}$, $\E{\compmeanvec_\icluster\compmeanvec_\icluster\T}=m_{\compmean}\bm{I}$, and $\E{\noisecov}=m_{\noisescalesq}\bm{I}$, which is good if the data is whitened and appropriate if the correlation structure is unknown a priori. Finally, we take $\E{\compweight_\icluster}=1/\numcluster$ which assumes that no preference is given to any cluster. Then, %We let $\lbrace\mixvec_{\ifactor}\rbrace_{\ifactor=1}^{\numfactor}$ be the columns of $\mix$, and then,
\begin{equation}
\Cov{\obsvec_n}=(m_{\scalesq}+m_{\compmean})\E{\mix\mix\T}+m_{\noisescalesq}\bm{I}
\end{equation}


All of the factor-based models we consider default to a $[\mix]_{ij}\sim\gaussrnd(0,1)$ prior, which results in the covariance
\begin{equation}
\Cov{\obsvec_n}=\left(\numfactor(m_{\scalesq}+m_{\compmean})+m_{\noisescalesq}\right)\bm{I}
\end{equation}
We will use the scalar \emph{average total variance} 
\begin{equation}
\frac{1}{\obsdim}\Tr{\Cov{\obsvec_n}}=\numfactor(m_{\scalesq}+m_{\compmean})+m_{\noisescalesq},
\end{equation}
as a calibration measure, with it roughly corresponding to the average variance along a dimension. As a standard Gaussian will have an average total variance of $1$, we use this as our calibration target, with the noise-level as an independent tuning parameter, resulting in the calibration relations
\begin{equation}
\label{eq:calibration}
m_{\compmean}+m_{\scalesq}=\frac{1-m_{\noisescalesq}}{\numfactor}.
\end{equation}
\subsubsection{Centered Independent Factor Analysis}
Since the cIFA is zero-mean, we have that $m_{\compmean}=0$, which through the calibration relation gives an immediate rule for setting the mean of $\compcov$. If we put identical priors on all the $\subscalesq$ then we should set the prior's mean $m_{\subscalesq}$ as per the rule,
\begin{equation}
m_{\subscalesq}=\frac{1-m_{\noisescalesq}}{\numfactor}.
\end{equation}
 A consequence of this would be that many of the univariate mixtures would have very similar components, resulting in roughly Gaussian source distributions. To avoid this, we impose a different prior on the first cluster of each univariate source mixture with mean $\rho {m}_{\subscalesq}$ for $\rho\in (0,1]$, and then ${m}_{\subscalesq}$ for the remaining clusters.

Since the clusters in this modified cIFA no longer have identical covariance, we recognize the alternative definition $m_{\scalesq}=\frac{1}{\numcluster}\sum_{c=1}^{\numcluster}\E{\compcov_c}$ in the calibration formula, and then calculate $m_{\scalesq}=(1-\frac{1-\rho}{\numifa}){m}_{\subscalesq}\bm{I}$, leading to the calibration relation,
\begin{equation}
{m}_{\subscalesq}=\frac{1-m_{\noisescalesq}}{\numfactor(1-\numifa^{-1}(1-\rho))}.
\end{equation}

\subsubsection{Projected Gaussian Mixture}
To calibrate the projected Gaussian mixture, we first introduce a tuning parameter $\rho$ that controls how much of the remaining variance should be explained by the cluster means, as opposed to the component variance, yielding the calibration relations, 
\begin{equation}
m_{\compmean}=\rho\frac{1-m_{\noisescalesq}}{\numfactor},\quad m_{\scalesq}=(1-\rho)\frac{1-m_{\noisescalesq}}{\numfactor},
\end{equation}
which jointly observe the original calibration relation of equation \eqref{eq:calibration}.

In the conjugate setting, $\compcov_\icluster$ follows an inverse Wishart $\mathcal{IW}(\nu, \kappa\bm{I})$ with degrees of freedom $\nu$ and scale matrix $\kappa\bm{I}$, and mean,
\begin{equation}
m_{\scalesq}=\frac{\kappa}{\nu - \numfactor - 1}\bm{I}.
\end{equation}
Since this leaves us with one constraint and two variables, we further note that $[\compcov_q]_{ii}$ is distributed as an inverse gamma,
\begin{equation}
[\compcov_q]_{ii}\sim \operatorname{IG}\left(\frac{\nu-K+1}{2},\frac{\kappa}{2}\right)
\end{equation}
with mean $\frac{\kappa}{\nu-K-1}$. Using this relationship, we can find parameters that lead to an appropriate marginal distribution by tuning
a new inverse Gamma $\operatorname{IG}(\alpha,\beta)$ to have mean $\frac{1-m_{\noisescalesq}}{2K}$ and appropriate variance following section \ref{sec:invgamvar}, and then setting
\begin{equation}
\nu=K+2\alpha-1,\quad \kappa=2\beta, 
\end{equation}
to ensure marginals with the same properties.
\subsubsection{Calibrating variances of inverse Gamma distributions}
\label{sec:invgamvar}
For an inverse Gamma distribution $\operatorname{InvGamma}(\alpha,\beta)$ the mean and variance does not exist unless $\alpha>2$, so we consider a random variable $X\sim\operatorname{InvGamma}(2+\alpha,\beta)$. $X$ then has mean and variance equal to 
\begin{align}
\E{X}=&\frac{\beta}{\alpha+1}\\
\Var{X}=&\frac{\beta^2}{\alpha(\alpha+1)^2}
\end{align}
If we set $\E{X}=m$ and $\Var{X}=v$, then we can isolate the alpha and beta parameters in terms of the mean and variance as,
\begin{align}
\beta=m\left(1+\frac{m^2}{v}\right),\quad 
\alpha=\frac{m^2}{v}
\end{align}



Employing Markov's inequality, we can bound the tail probability using both the mean and the variance as 
\begin{equation}
\mathbb{P}[X\geq a]\leq \frac{\E{X}}{a},\quad \mathbb{P}[X\geq a]\leq \frac{\Var{X}}{(a-\E{X})^2}.
\end{equation}
Assume that the mean is set equal to $m$. Then if we want less than $t$ mass in the tails, we can select the variance to be
\begin{equation}
\Var{X}=t(a-m)^2
\end{equation}

%For a noise variable, we are likely to want a low mean variance like $m=10^{-1}$, and we likely want to contain (at least) $1-t=0.95$ of the probability mass to $[0,1]$ requiring $a=1$. This results in a proposed variance of 
%\begin{equation}
%\Var{X}=\frac{1}{20}(1-10^{-1})^2=0.0405
%\end{equation} 
%this roughly holds if $\beta=0.1247$ and $\alpha=0.2469$ (or $\alpha=2.2469$ in the original parameterization).


\end{document}