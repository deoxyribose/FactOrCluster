\documentclass{article}
\input{preambles/basicmath}
\input{preambles/extrasymbols}

\newcommand{\mix}{\bm{A}}
\newcommand{\source}{s}
\newcommand{\sourcevec}{\bm{\source}}
\newcommand{\obs}{y}
\newcommand{\obsvec}{\bm{\obs}}
\newcommand{\mean}{\mu}
\newcommand{\meanvec}{\bm{\mean}}
\newcommand{\scale}{\sigma}
\newcommand{\scalesq}{\scale^2}
\newcommand{\weight}{w}
\newcommand{\weightvec}{\bm{\weight}}
\newcommand{\compweight}{\bar{\weight}}
\newcommand{\compweightvec}{\bar{\bm{\weight}}}
\newcommand{\assign}{z}
\newcommand{\assignvec}{\bm{\assign}}
\newcommand{\compmean}{\bar{\mean}}
\newcommand{\compmeanvec}{\bar{\meanvec}}
\newcommand{\compcov}{\bar{\bm{\Sigma}}}
\newcommand{\compscalesq}{\bar{{\scale}}^2}
\newcommand{\noisecov}{\bm{\Lambda}}
\newcommand{\noisescalesq}{\lambda^2}

\begin{document}
The Independent Factor Analysis (IFA) model assumes a standard generative factor model for the observations $\bm{y}$ with $K$ factors,  
\begin{align*}
\obsvec_n\sim \gaussrnd(\mix\sourcevec_n,\noisecov),
\end{align*}
but uses an expressive factorial mixture of Gaussians for the sources with $C$ components per factor $k$,
\begin{align*}
\source_{nk}\sim \sum_{c=1}^{C}\weight_{kc}\gaussrnd(\mean_{kc},\scalesq_{kc}),
\end{align*}

By multiplying the univariate mixtures together, we can equivalently state that the multivariate source vector $\sourcevec$ is drawn from a mixture of Gaussians with diagonal covariance and $Q=C^K$ components, 
\begin{equation}
\sourcevec_n\sim\sum_{q=1}^Q\compweight_{q}\gaussrnd(\compmeanvec_{q},\compcov_q).
\end{equation}
As with any mixture, we can express the mixture density using auxiliary indicators, where $\assign_{nq}=1$ if observation $n$ was drawn from component $q$, and $0$ otherwise.
\begin{equation}
\sourcevec_n\cond \assignvec_n\sim\prod_{q=1}^Q\gaussrnd(\compmeanvec_{q},\compcov_q)^{\assign_{nq}},\quad \assignvec_n\sim \multrnd(\compweightvec).
\end{equation}

with $N$ observations, the full log-joint has terms
\begin{align*}
\ln p(\obsvec|\sourcevec)&=\sum_{n=1}^{N}\left(-\frac{1}{2}(\obsvec_n-\mix\sourcevec_n)\T\noisecov^{-1}(\obsvec_n-\mix\sourcevec_n)-\frac{1}{2}\ln|\noisecov|-\frac{D}{2}\ln 2\pi)\right)\\
\ln p(\sourcevec|\assignvec)&=\sum_{n=1}^{N}\sum_{q=1}^{Q}\assign_{nq}\left(-\frac{1}{2}(\sourcevec_n-\compmeanvec_q)\T\compcov^{-1}_q(\sourcevec_n-\compmeanvec_q)-\frac{1}{2}\ln|\compcov_q|-\frac{D}{2}\ln 2\pi)\right)\\
\ln p(\assignvec)&=\sum_{n=1}^{N}\sum_{q=1}^{Q}\assign_{nq}\ln \compweight_{q}
\end{align*}
To compute gradients in $\ln p(\obsvec)$, we need to be able to integrate $\sourcevec$ and $\assignvec$ over $p(\sourcevec,\assignvec|\obsvec)=p(\sourcevec|\assignvec,\obsvec)p(\assignvec|\obsvec)$. First we have a standard Gaussian posterior,
\begin{equation}
p(\sourcevec_n|\assign_{nq}=1,\obsvec_n)=\gaussrnd(\bm{V}_q\mix\T\noisecov^{-1}\obsvec_n,\bm{V}_q),\quad \bm{V}_q=(\mix\T\noisecov^{-1}\mix+\compcov^{-1}_q)^{-1}
\end{equation}
and then using a standard responsibility argument, we can find the posterior of the assignment indicators $\assignvec$ as
\begin{equation}
p(\assignvec_{nq}=1|\obsvec_n)\propto \compweight_{q}p(\obsvec_n|\assign_{nq}=1)=\compweight_{q}\gaussrnd(\obsvec_n|\mix\compmeanvec_q,\noisecov+\mix\compcov_q\mix\T).
\end{equation}
\subsection{Mixture weights}
Since $\sum_{c=1}^{C}\weight_{kc}=1$, we have to add Lagrangians, and due to the positivity constraint we reparameterize as $v_{kc}=\ln\weight_{kc}$. The gradient is then  
\begin{equation}
\nabla_{v_{kc}}\left(\ln p(\obsvec,\sourcevec,\assignvec) +\sum_{\ell=1}^{K}\rho_\ell(1-\sum_{d=1}^{C}e^{v_{\ell d}})\right)=\sum_{n=1}^N\sum_{q=1}^Q\assign_{nq}\xi_{kc}^q-\rho_ke^{v_{kc}}
\end{equation}
Taking the expectation and solving we find,
\begin{equation}
\weight_{kc}\propto\sum_{n=1}^{N}\sum_{q=1}^Q\xi_{kc}^q\E{\assign_{nq}}
\end{equation}
If we add a Dirichlet prior $\weightvec_k\sim\dirrnd(\bm{\alpha}_k)$, then it contributes with the terms 
\begin{equation}
\nabla_{v_{kc}}\ln p(\weightvec_k)=\alpha_{kc}-1
\end{equation}
which changes the analytical solution to
\begin{equation}
\weight_{kc}\propto (\alpha_{kc}-1)+\sum_{n=1}^{N}\sum_{q=1}^Q\xi_{kc}^q\E{\assign_{nq}}
\end{equation}

\subsection{Mixture Variance}
Taking the gradient in $\compcov_q$, we get
\begin{equation}
\nabla_{\compcov_q}\ln p(\obsvec,\sourcevec,\assignvec)=-\frac{1}{2}\sum_{n=1}^{N}\assign_{nq}\left(\compcov_q^{-1}(\sourcevec_n-\compmeanvec_q)(\sourcevec_n-\compmeanvec_q)\T\compcov_q^{-1}-\compcov^{-1}_q\right)
\end{equation}
Let the diagonal elements be denoted by $\compscalesq_{qk}=[\compcov_q]_{kk}$.
We take $\xi_{kc}^q$ to be an indicator of whether $\scalesq_{kc}$ is a factor of $\compscalesq_{qk}$. Then we can use the chain rule to get
\begin{equation}
\pdv{\scalesq_{kc}}\ln p(\obsvec,\sourcevec,\assignvec)=-\frac{1}{2}\sum_{q=1}^{Q}\xi_{kc}^{q}\left(\sum_{n=1}^{N}\assign_{nq}\left(\frac{\source_{nk}-\compmean_{qk}}{\compscalesq_{qk}}\right)^2-\frac{\sum_{n=1}^{N}\assign_{nq}}{\compscalesq_{qk}}\right)
\end{equation}
We can get the gradient in $\ln p(\obsvec)$ by taking the expectation of the above quantity with respect to the posteriors we calculated before, and we can then solve it analytically,
\begin{equation}
\scalesq_{kc} =\frac{\sum_{q=1}^{Q}\sum_{n=1}^{N}\xi_{kc}^q\E{\assign_{nq}}\E{(\source_{nk}-\compmean_{qk})^2}}{\sum_{q=1}^{Q}\sum_{n=1}^{N}\xi_{kc}^q\E{\assign_{nq}}}
\end{equation}
If $\scalesq_{kc}$ is endowed with an inverse Gamma prior with shape $a_{kc}$ and scale $b_{kc}$,
\begin{equation}
\ln p(\scalesq_{kc})=-(1+a_{kc})\ln\noisescalesq_{kc}-\frac{b_{kc}}{\noisescalesq_{kc}}
\end{equation}
then it contributes the terms,
\begin{equation}
\nabla_{\scalesq}\ln p(\scalesq_{kc})=-(1+a_{kc})\frac{1}{\scalesq_{kc}}+\frac{b_{kc}}{\scalesq_{kc}}
\end{equation}
and adding those terms to the gradient we can solve again and find
\begin{equation}
\scalesq_{kc} =\frac{2(1+a_{kc})+\sum_{q=1}^{Q}\sum_{n=1}^{N}\xi_{kc}^q\E{\assign_{nq}}\E{(\source_{nk}-\compmean_{qk})^2}}{2b_{kc}+\sum_{q=1}^{Q}\sum_{n=1}^{N}\xi_{kc}^q\E{\assign_{nq}}}
\end{equation}
\subsection{Noise Variance}
\begin{equation}
\nabla_{\noisecov}\ln p(\obsvec,\sourcevec,\assignvec)=\frac{1}{2}\sum_{n=1}^{N}\left(\noisecov^{-1}(\obsvec_n-\mix\sourcevec_n)(\obsvec_n-\mix\sourcevec_n)\T\noisecov^{-1}-\noisecov^{-1}\right)
\end{equation}
Taking the diagonal elements $\noisescalesq_k$
\begin{equation}
\pdv{\noisescalesq_{k}}\ln p(\obsvec,\sourcevec,\assignvec)=\frac{1}{2}\left(\sum_{n=1}^{N}\left(\frac{\obs_{nk}-\bm{e}_k\T\mix\sourcevec_n}{\noisescalesq_{k}}\right)^2-N\frac{1}{\noisescalesq_k}\right)
\end{equation}
Taking the expectation and solving, we get
\begin{equation}
\noisescalesq_k=\frac{1}{N}\sum_{n=1}^{N}\E{(\obs_{nk}-\bm{e}_k\T\mix\sourcevec_n)^2}=\frac{1}{N}\sum_{n=1}^{N}(\obs_{nk}^2+\bm{e}_k\T\mix\E{\sourcevec_n\sourcevec_n\T}\mix\T\bm{e}_k-2\obs_{nk}\bm{e}_k\T\mix\E{\sourcevec_n})
\end{equation}
If $\noisescalesq_k$ is again endowed with an inverse Gamma prior then it contributes the terms,
\begin{equation}
\nabla_{\noisescalesq}\ln p(\noisescalesq_k)=-(1+\alpha_k)\frac{1}{\noisescalesq_k}+\frac{\beta_k}{(\noisescalesq_k)^2}
\end{equation}
and adding those terms to the gradient we can solve again and find
\begin{equation*}
\noisescalesq_k=\frac{1}{N+2+2\alpha_k}\sum_{n=1}^{N}\left(2\beta_k+\E{(\obs_{nk}-\bm{e}_k\T\mix\sourcevec_n)^2}\right)
\end{equation*}
If we have a single $\noisescalesq_0$ controlling the noise level (scaled unit diagonal covariance), then the gradient simplifies
\begin{equation}
\pdv{\noisescalesq_{0}}\ln p(\obsvec,\sourcevec,\assignvec)=\frac{1}{(\noisescalesq_0)^2}\left(\frac{1}{2}\sum_{n=1}^{N}(\obsvec_n-\mix\sourcevec_n)\T(\obsvec_n-\mix\sourcevec_n)\right)-\frac{ND}{2}\frac{1}{\noisescalesq_0}
%-\frac{1}{2\noisescalesq_0}\left(\frac{1}{\noisescalesq_0}\sum_{n=1}^{N}\left(\sum_{k=1}^K\obs_{nk}-\Tr[\mix\E{\sourcevec_n\sourcevec_n\T}\mix\T]-2\obsvec_n\T\mix\E{\sourcevec_n}\right)-NK\right)
\end{equation}
and taking the expectation and isolating yields,
\begin{equation}
\noisescalesq_0=\frac{\sum_{n=1}^{N}(\obsvec_n\T\obsvec_n+\Trace(\mix\T\mix\E{\sourcevec_n\sourcevec_n\T})-2\obsvec_n\T\mix\E{\sourcevec_n})}{ND}
\end{equation}
or if $\noisecov$ is a scaled unit matrix and 
\begin{equation}
\noisescalesq_0=\frac{2\beta_0+\sum_{n=1}^{N}(\obsvec_n\T\obsvec_n+\Trace(\mix\T\mix\E{\sourcevec_n\sourcevec_n\T})-2\obsvec_n\T\mix\E{\sourcevec_n})}{ND+2+2\alpha_0}
\end{equation}

\subsection{Factor Loadings}
\begin{equation}
\nabla_{\mix}\ln p(\obsvec,\sourcevec,\assignvec)=\noisecov^{-1}\sum_{n=1}^{N}(\obsvec_n-\mix\sourcevec_n)\sourcevec_n\T
\end{equation}
we can then again take the expectation, 
\begin{equation}
\E{\nabla_{\mix}\ln p(\obsvec,\sourcevec,\assignvec)}=\noisecov^{-1}\left(\sum_{n=1}^{N}\obsvec_n\E{\sourcevec_n}\T-\mix\left(\sum_{n=1}^{N}\E{\sourcevec_n\sourcevec_n\T}\right)\right)
\end{equation}
and set the gradient to zero to find the optimal update
\begin{equation*}
\mix = \left(\sum_{n=1}^{N}\obsvec_n\E{\sourcevec_n}\T\right)\left(\sum_{n=1}^{N}\E{\sourcevec_n\sourcevec_n\T}\right)^{-1}
\end{equation*}
If we add a Gaussian prior $[\mix]_{ij}\sim \gaussrnd(0,\scalesq_{\mix})$ it contributes the term,
\begin{equation*}
\E{\nabla_{\mix}\ln p(\mix)}=-\frac{1}{\scalesq_{\mix}} \mix
\end{equation*}
and we can solve again to find the MAP update,
\begin{equation*}
\mix = \left(\sum_{n=1}^{N}\obsvec_n\E{\sourcevec_n}\T\right)\left(\sum_{n=1}^{N}\E{\sourcevec_n\sourcevec_n\T}+\frac{\noisescalesq}{\scalesq_{\mix}}\right)^{-1}
\end{equation*}





\end{document}